{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scifa\\Anaconda2\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# data pre-processing\n",
    "data = pd.read_csv('./data/AllMoviesDetailsCleaned.csv', sep=';', low_memory=False)\n",
    "data = data[data.budget != 0]\n",
    "data = data[data.revenue != 0]\n",
    "\n",
    "X = data[['budget', 'popularity']]\n",
    "X[\"popularity\"] = X[\"popularity\"].str.replace(\",\",\".\").astype(float)\n",
    "Y = data.loc[:, 'revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_logarithmic_error', optimizer=SGD(lr=0.001, momentum=0.9))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    pyplot.title('Loss')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4788 samples, validate on 1198 samples\n",
      "Epoch 1/100\n",
      "4788/4788 [==============================] - 4s 902us/step - loss: 4.7279 - val_loss: 3.1058\n",
      "Epoch 2/100\n",
      "4788/4788 [==============================] - 1s 198us/step - loss: 4.1115 - val_loss: 3.1014\n",
      "Epoch 3/100\n",
      "4788/4788 [==============================] - 1s 206us/step - loss: 4.1100 - val_loss: 3.1016\n",
      "Epoch 4/100\n",
      "4788/4788 [==============================] - 1s 211us/step - loss: 4.1122 - val_loss: 3.1101\n",
      "Epoch 5/100\n",
      "4788/4788 [==============================] - 1s 227us/step - loss: 4.1079 - val_loss: 3.1033\n",
      "Epoch 6/100\n",
      "4788/4788 [==============================] - 1s 198us/step - loss: 4.1028 - val_loss: 3.1014\n",
      "Epoch 7/100\n",
      "4788/4788 [==============================] - 1s 199us/step - loss: 4.1039 - val_loss: 3.1118\n",
      "Epoch 8/100\n",
      "4788/4788 [==============================] - 1s 190us/step - loss: 4.1027 - val_loss: 3.1063\n",
      "Epoch 9/100\n",
      "4788/4788 [==============================] - 1s 291us/step - loss: 4.1023 - val_loss: 3.1008\n",
      "Epoch 10/100\n",
      "4788/4788 [==============================] - 1s 194us/step - loss: 4.1005 - val_loss: 3.1024\n",
      "Epoch 11/100\n",
      "4788/4788 [==============================] - 1s 199us/step - loss: 4.1001 - val_loss: 3.1007\n",
      "Epoch 12/100\n",
      "4788/4788 [==============================] - 1s 198us/step - loss: 4.1027 - val_loss: 3.1006\n",
      "Epoch 13/100\n",
      "4788/4788 [==============================] - 1s 202us/step - loss: 4.0941 - val_loss: 3.1006\n",
      "Epoch 14/100\n",
      "4788/4788 [==============================] - 1s 212us/step - loss: 4.1036 - val_loss: 3.1065\n",
      "Epoch 15/100\n",
      "4788/4788 [==============================] - 1s 230us/step - loss: 4.0941 - val_loss: 3.1040\n",
      "Epoch 16/100\n",
      "4788/4788 [==============================] - 1s 187us/step - loss: 4.0931 - val_loss: 3.1039\n",
      "Epoch 17/100\n",
      "4788/4788 [==============================] - 1s 208us/step - loss: 4.0954 - val_loss: 3.1011\n",
      "Epoch 18/100\n",
      "4788/4788 [==============================] - 1s 195us/step - loss: 4.0900 - val_loss: 3.1359\n",
      "Epoch 19/100\n",
      "4788/4788 [==============================] - 1s 214us/step - loss: 4.0913 - val_loss: 3.1018\n",
      "Epoch 20/100\n",
      "4788/4788 [==============================] - 1s 216us/step - loss: 4.0961 - val_loss: 3.1004\n",
      "Epoch 21/100\n",
      "4788/4788 [==============================] - 1s 274us/step - loss: 4.0880 - val_loss: 3.1075\n",
      "Epoch 22/100\n",
      "4788/4788 [==============================] - 2s 345us/step - loss: 4.0881 - val_loss: 3.1007\n",
      "Epoch 23/100\n",
      "4788/4788 [==============================] - 2s 355us/step - loss: 4.0901 - val_loss: 3.1129\n",
      "Epoch 24/100\n",
      "4788/4788 [==============================] - 1s 240us/step - loss: 4.0886 - val_loss: 3.1045\n",
      "Epoch 25/100\n",
      "4788/4788 [==============================] - 1s 252us/step - loss: 4.0887 - val_loss: 3.1004\n",
      "Epoch 26/100\n",
      "4788/4788 [==============================] - 1s 245us/step - loss: 4.0866 - val_loss: 3.1033\n",
      "Epoch 27/100\n",
      "4788/4788 [==============================] - 1s 258us/step - loss: 4.0896 - val_loss: 3.1239\n",
      "Epoch 28/100\n",
      "4788/4788 [==============================] - 1s 218us/step - loss: 4.0863 - val_loss: 3.1031\n",
      "Epoch 29/100\n",
      "4788/4788 [==============================] - 1s 238us/step - loss: 4.0820 - val_loss: 3.1204\n",
      "Epoch 30/100\n",
      "4788/4788 [==============================] - 1s 224us/step - loss: 4.0853 - val_loss: 3.1040\n",
      "Epoch 31/100\n",
      "4788/4788 [==============================] - 1s 237us/step - loss: 4.0838 - val_loss: 3.1008\n",
      "Epoch 32/100\n",
      "4788/4788 [==============================] - 1s 268us/step - loss: 4.0835 - val_loss: 3.1112\n",
      "Epoch 33/100\n",
      "4788/4788 [==============================] - 1s 225us/step - loss: 4.0831 - val_loss: 3.1015\n",
      "Epoch 34/100\n",
      "4788/4788 [==============================] - 1s 218us/step - loss: 4.0838 - val_loss: 3.1183\n",
      "Epoch 35/100\n",
      "4788/4788 [==============================] - 2s 421us/step - loss: 4.0844 - val_loss: 3.1073\n",
      "Epoch 36/100\n",
      "4788/4788 [==============================] - 1s 209us/step - loss: 4.0800 - val_loss: 3.1003\n",
      "Epoch 37/100\n",
      "4788/4788 [==============================] - 1s 257us/step - loss: 4.0834 - val_loss: 3.1056\n",
      "Epoch 38/100\n",
      "4788/4788 [==============================] - 1s 274us/step - loss: 4.0808 - val_loss: 3.1001\n",
      "Epoch 39/100\n",
      "4788/4788 [==============================] - ETA: 0s - loss: 4.0906- ETA: 0s - loss: 4.01 - 1s 287us/step - loss: 4.0815 - val_loss: 3.1014\n",
      "Epoch 40/100\n",
      "4788/4788 [==============================] - 2s 457us/step - loss: 4.0777 - val_loss: 3.1165\n",
      "Epoch 41/100\n",
      "4788/4788 [==============================] - 1s 272us/step - loss: 4.0791 - val_loss: 3.1023\n",
      "Epoch 42/100\n",
      "4788/4788 [==============================] - 1s 241us/step - loss: 4.0762 - val_loss: 3.1082\n",
      "Epoch 43/100\n",
      "4788/4788 [==============================] - 1s 245us/step - loss: 4.0776 - val_loss: 3.1065\n",
      "Epoch 44/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 4.0771 - val_loss: 3.1003\n",
      "Epoch 45/100\n",
      "4788/4788 [==============================] - 1s 310us/step - loss: 4.0749 - val_loss: 3.1022\n",
      "Epoch 46/100\n",
      "4788/4788 [==============================] - 1s 272us/step - loss: 4.0785 - val_loss: 3.1029\n",
      "Epoch 47/100\n",
      "4788/4788 [==============================] - 1s 233us/step - loss: 4.0742 - val_loss: 3.1057\n",
      "Epoch 48/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 4.0746 - val_loss: 3.1056\n",
      "Epoch 49/100\n",
      "4788/4788 [==============================] - 1s 233us/step - loss: 4.0779 - val_loss: 3.1188\n",
      "Epoch 50/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 4.0734 - val_loss: 3.1007\n",
      "Epoch 51/100\n",
      "4788/4788 [==============================] - 1s 224us/step - loss: 4.0697 - val_loss: 3.1031\n",
      "Epoch 52/100\n",
      "4788/4788 [==============================] - 2s 365us/step - loss: 4.0712 - val_loss: 3.1066\n",
      "Epoch 53/100\n",
      "4788/4788 [==============================] - 2s 350us/step - loss: 4.0716 - val_loss: 3.1042A: 0s - lo - ETA: 0s - loss: 4.055\n",
      "Epoch 54/100\n",
      "4788/4788 [==============================] - 2s 324us/step - loss: 4.0726 - val_loss: 3.1058\n",
      "Epoch 55/100\n",
      "4788/4788 [==============================] - 1s 267us/step - loss: 4.0699 - val_loss: 3.1006\n",
      "Epoch 56/100\n",
      "4788/4788 [==============================] - 1s 267us/step - loss: 4.0684 - val_loss: 3.1029\n",
      "Epoch 57/100\n",
      "4788/4788 [==============================] - 1s 269us/step - loss: 4.0687 - val_loss: 3.1017\n",
      "Epoch 58/100\n",
      "4788/4788 [==============================] - 1s 260us/step - loss: 4.0710 - val_loss: 3.1005\n",
      "Epoch 59/100\n",
      "4788/4788 [==============================] - 2s 328us/step - loss: 4.0706 - val_loss: 3.1009\n",
      "Epoch 60/100\n",
      "4788/4788 [==============================] - 2s 389us/step - loss: 4.0665 - val_loss: 3.1007\n",
      "Epoch 61/100\n",
      "4788/4788 [==============================] - 2s 318us/step - loss: 4.0664 - val_loss: 3.1088\n",
      "Epoch 62/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 4.0638 - val_loss: 3.1119\n",
      "Epoch 63/100\n",
      "4788/4788 [==============================] - 2s 341us/step - loss: 4.0655 - val_loss: 3.1010ss: 4.085\n",
      "Epoch 64/100\n",
      "4788/4788 [==============================] - 2s 386us/step - loss: 4.0694 - val_loss: 3.1026\n",
      "Epoch 65/100\n",
      "4788/4788 [==============================] - 1s 283us/step - loss: 4.0679 - val_loss: 3.1056\n",
      "Epoch 66/100\n",
      "4788/4788 [==============================] - 1s 258us/step - loss: 4.0647 - val_loss: 3.1016\n",
      "Epoch 67/100\n",
      "4788/4788 [==============================] - 2s 350us/step - loss: 4.0660 - val_loss: 3.1103\n",
      "Epoch 68/100\n",
      "4788/4788 [==============================] - 2s 362us/step - loss: 4.0635 - val_loss: 3.1011\n",
      "Epoch 69/100\n",
      "4788/4788 [==============================] - 2s 317us/step - loss: 4.0647 - val_loss: 3.1125\n",
      "Epoch 70/100\n",
      "4788/4788 [==============================] - 1s 277us/step - loss: 4.0610 - val_loss: 3.1050\n",
      "Epoch 71/100\n",
      "4788/4788 [==============================] - 1s 309us/step - loss: 4.0633 - val_loss: 3.1022\n",
      "Epoch 72/100\n",
      "4788/4788 [==============================] - 1s 302us/step - loss: 4.0612 - val_loss: 3.1138\n",
      "Epoch 73/100\n",
      "4788/4788 [==============================] - 1s 286us/step - loss: 4.0613 - val_loss: 3.1044\n",
      "Epoch 74/100\n",
      "4788/4788 [==============================] - 2s 351us/step - loss: 4.0662 - val_loss: 3.1016\n",
      "Epoch 75/100\n",
      "4788/4788 [==============================] - 2s 313us/step - loss: 4.0637 - val_loss: 3.1024\n",
      "Epoch 76/100\n",
      "4788/4788 [==============================] - 2s 337us/step - loss: 4.0593 - val_loss: 3.1041\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4788/4788 [==============================] - 1s 299us/step - loss: 4.0611 - val_loss: 3.1015\n",
      "Epoch 78/100\n",
      "4788/4788 [==============================] - 1s 258us/step - loss: 4.0549 - val_loss: 3.1074\n",
      "Epoch 79/100\n",
      "4788/4788 [==============================] - 1s 227us/step - loss: 4.0586 - val_loss: 3.1021\n",
      "Epoch 80/100\n",
      "4788/4788 [==============================] - 2s 334us/step - loss: 4.0577 - val_loss: 3.1046\n",
      "Epoch 81/100\n",
      "4788/4788 [==============================] - 2s 446us/step - loss: 4.0573 - val_loss: 3.1040\n",
      "Epoch 82/100\n",
      "4788/4788 [==============================] - 1s 243us/step - loss: 4.0585 - val_loss: 3.1025\n",
      "Epoch 83/100\n",
      "4788/4788 [==============================] - 1s 218us/step - loss: 4.0589 - val_loss: 3.1106\n",
      "Epoch 84/100\n",
      "4788/4788 [==============================] - 1s 206us/step - loss: 4.0592 - val_loss: 3.1086\n",
      "Epoch 85/100\n",
      "4788/4788 [==============================] - 1s 216us/step - loss: 4.0577 - val_loss: 3.1036\n",
      "Epoch 86/100\n",
      "4788/4788 [==============================] - 1s 187us/step - loss: 4.0582 - val_loss: 3.1029\n",
      "Epoch 87/100\n",
      "4788/4788 [==============================] - 1s 235us/step - loss: 4.0530 - val_loss: 3.1283\n",
      "Epoch 88/100\n",
      "4788/4788 [==============================] - 2s 408us/step - loss: 4.0583 - val_loss: 3.1028\n",
      "Epoch 89/100\n",
      "4788/4788 [==============================] - 1s 283us/step - loss: 4.0543 - val_loss: 3.1037\n",
      "Epoch 90/100\n",
      "4788/4788 [==============================] - 2s 387us/step - loss: 4.0575 - val_loss: 3.1099\n",
      "Epoch 91/100\n",
      "4788/4788 [==============================] - 1s 307us/step - loss: 4.0539 - val_loss: 3.1029\n",
      "Epoch 92/100\n",
      "4788/4788 [==============================] - 1s 221us/step - loss: 4.0537 - val_loss: 3.1040\n",
      "Epoch 93/100\n",
      "4788/4788 [==============================] - 1s 265us/step - loss: 4.0546 - val_loss: 3.1079\n",
      "Epoch 94/100\n",
      "4788/4788 [==============================] - 1s 231us/step - loss: 4.0542 - val_loss: 3.1032\n",
      "Epoch 95/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 4.0534 - val_loss: 3.1042\n",
      "Epoch 96/100\n",
      "4788/4788 [==============================] - 1s 253us/step - loss: 4.0530 - val_loss: 3.1078\n",
      "Epoch 97/100\n",
      "4788/4788 [==============================] - 1s 248us/step - loss: 4.0517 - val_loss: 3.1153\n",
      "Epoch 98/100\n",
      "4788/4788 [==============================] - 2s 390us/step - loss: 4.0526 - val_loss: 3.1036\n",
      "Epoch 99/100\n",
      "4788/4788 [==============================] - 1s 288us/step - loss: 4.0517 - val_loss: 3.1072\n",
      "Epoch 100/100\n",
      "4788/4788 [==============================] - 1s 236us/step - loss: 4.0546 - val_loss: 3.1104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model = baseline_model()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # Compile model\n",
    "    opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "    model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4788 samples, validate on 1198 samples\n",
      "Epoch 1/100\n",
      "4788/4788 [==============================] - 7s 2ms/step - loss: 29.9287 - acc: 4.1771e-04 - val_loss: 23.1560 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "4788/4788 [==============================] - 1s 301us/step - loss: 17.9377 - acc: 0.0010 - val_loss: 16.2681 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "4788/4788 [==============================] - 2s 355us/step - loss: 12.9920 - acc: 0.0054 - val_loss: 12.5715 - val_acc: 0.0017\n",
      "Epoch 4/100\n",
      "4788/4788 [==============================] - 2s 320us/step - loss: 10.1885 - acc: 0.0048 - val_loss: 10.3053 - val_acc: 0.0017\n",
      "Epoch 5/100\n",
      "4788/4788 [==============================] - 2s 321us/step - loss: 8.4151 - acc: 0.0048 - val_loss: 8.7917 - val_acc: 8.3472e-04\n",
      "Epoch 6/100\n",
      "4788/4788 [==============================] - 2s 324us/step - loss: 7.2090 - acc: 0.0017 - val_loss: 7.7307 - val_acc: 0.0017\n",
      "Epoch 7/100\n",
      "4788/4788 [==============================] - 2s 330us/step - loss: 6.3530 - acc: 0.0013 - val_loss: 6.9531 - val_acc: 8.3472e-04\n",
      "Epoch 8/100\n",
      "4788/4788 [==============================] - 2s 349us/step - loss: 5.7250 - acc: 8.3542e-04 - val_loss: 6.3713 - val_acc: 0.0017\n",
      "Epoch 9/100\n",
      "4788/4788 [==============================] - 2s 313us/step - loss: 5.2553 - acc: 6.2657e-04 - val_loss: 5.9242 - val_acc: 0.0025\n",
      "Epoch 10/100\n",
      "4788/4788 [==============================] - 2s 324us/step - loss: 4.8992 - acc: 6.2657e-04 - val_loss: 5.5807 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "4788/4788 [==============================] - 2s 320us/step - loss: 4.6269 - acc: 8.3542e-04 - val_loss: 5.3130 - val_acc: 0.0033\n",
      "Epoch 12/100\n",
      "4788/4788 [==============================] - 2s 338us/step - loss: 4.4165 - acc: 0.0015 - val_loss: 5.1037 - val_acc: 0.0017ETA: 1s - loss:\n",
      "Epoch 13/100\n",
      "4788/4788 [==============================] - 2s 402us/step - loss: 4.2535 - acc: 4.1771e-04 - val_loss: 4.9370 - val_acc: 8.3472e-04\n",
      "Epoch 14/100\n",
      "4788/4788 [==============================] - 2s 320us/step - loss: 4.1268 - acc: 4.1771e-04 - val_loss: 4.8043 - val_acc: 8.3472e-04\n",
      "Epoch 15/100\n",
      "4788/4788 [==============================] - 2s 328us/step - loss: 4.0289 - acc: 0.0000e+00 - val_loss: 4.7002 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "4788/4788 [==============================] - 1s 304us/step - loss: 3.9533 - acc: 0.0015 - val_loss: 4.6180 - val_acc: 0.0017\n",
      "Epoch 17/100\n",
      "4788/4788 [==============================] - 2s 374us/step - loss: 3.8951 - acc: 8.3542e-04 - val_loss: 4.5525 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "4788/4788 [==============================] - 2s 419us/step - loss: 3.8511 - acc: 0.0000e+00 - val_loss: 4.5004 - val_acc: 8.3472e-04\n",
      "Epoch 19/100\n",
      "4788/4788 [==============================] - 2s 428us/step - loss: 3.8178 - acc: 0.0000e+00 - val_loss: 4.4604 - val_acc: 8.3472e-04\n",
      "Epoch 20/100\n",
      "4788/4788 [==============================] - 2s 400us/step - loss: 3.7931 - acc: 2.0886e-04 - val_loss: 4.4286 - val_acc: 8.3472e-04\n",
      "Epoch 21/100\n",
      "4788/4788 [==============================] - 1s 299us/step - loss: 3.7749 - acc: 4.1771e-04 - val_loss: 4.4053 - val_acc: 8.3472e-04\n",
      "Epoch 22/100\n",
      "4788/4788 [==============================] - 2s 486us/step - loss: 3.7618 - acc: 4.1771e-04 - val_loss: 4.3849 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "4788/4788 [==============================] - 2s 422us/step - loss: 3.7523 - acc: 8.3542e-04 - val_loss: 4.3712 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "4788/4788 [==============================] - 2s 398us/step - loss: 3.7457 - acc: 6.2657e-04 - val_loss: 4.3599 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "4788/4788 [==============================] - 1s 310us/step - loss: 3.7412 - acc: 2.0886e-04 - val_loss: 4.3509 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "4788/4788 [==============================] - 2s 332us/step - loss: 3.7383 - acc: 0.0000e+00 - val_loss: 4.3441 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "4788/4788 [==============================] - 2s 342us/step - loss: 3.7365 - acc: 0.0000e+00 - val_loss: 4.3398 - val_acc: 8.3472e-04\n",
      "Epoch 28/100\n",
      "4788/4788 [==============================] - 1s 306us/step - loss: 3.7351 - acc: 0.0000e+00 - val_loss: 4.3354 - val_acc: 8.3472e-04\n",
      "Epoch 29/100\n",
      "4788/4788 [==============================] - 2s 346us/step - loss: 3.7344 - acc: 0.0000e+00 - val_loss: 4.3331 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "4788/4788 [==============================] - 2s 314us/step - loss: 3.7339 - acc: 0.0000e+00 - val_loss: 4.3304 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "4788/4788 [==============================] - 2s 349us/step - loss: 3.7337 - acc: 0.0000e+00 - val_loss: 4.3284 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "4788/4788 [==============================] - 2s 410us/step - loss: 3.7335 - acc: 0.0000e+00 - val_loss: 4.3278 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "4788/4788 [==============================] - 1s 254us/step - loss: 3.7333 - acc: 0.0000e+00 - val_loss: 4.3269 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 3.7332 - acc: 0.0000e+00 - val_loss: 4.3259 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "4788/4788 [==============================] - 1s 255us/step - loss: 3.7332 - acc: 0.0000e+00 - val_loss: 4.3262 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "4788/4788 [==============================] - 1s 257us/step - loss: 3.7331 - acc: 0.0000e+00 - val_loss: 4.3252 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "4788/4788 [==============================] - ETA: 0s - loss: 3.7258 - acc: 0.0000e+0 - 1s 242us/step - loss: 3.7330 - acc: 0.0000e+00 - val_loss: 4.3243 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "4788/4788 [==============================] - 1s 249us/step - loss: 3.7330 - acc: 0.0000e+00 - val_loss: 4.3236 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "4788/4788 [==============================] - 1s 245us/step - loss: 3.7328 - acc: 0.0000e+00 - val_loss: 4.3236 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "4788/4788 [==============================] - 1s 260us/step - loss: 3.7328 - acc: 0.0000e+00 - val_loss: 4.3233 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "4788/4788 [==============================] - ETA: 0s - loss: 3.6938 - acc: 0.0000e+00- ETA: 0s - loss: 3.6484 - acc: 0. - 1s 238us/step - loss: 3.7327 - acc: 0.0000e+00 - val_loss: 4.3226 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "4788/4788 [==============================] - 1s 249us/step - loss: 3.7328 - acc: 0.0000e+00 - val_loss: 4.3223 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 3.7325 - acc: 0.0000e+00 - val_loss: 4.3226 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "4788/4788 [==============================] - 1s 245us/step - loss: 3.7327 - acc: 0.0000e+00 - val_loss: 4.3223 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "4788/4788 [==============================] - 1s 261us/step - loss: 3.7325 - acc: 0.0000e+00 - val_loss: 4.3230 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 3.7323 - acc: 0.0000e+00 - val_loss: 4.3217 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "4788/4788 [==============================] - 1s 240us/step - loss: 3.7323 - acc: 2.0886e-04 - val_loss: 4.3213 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "4788/4788 [==============================] - 1s 225us/step - loss: 3.7324 - acc: 0.0000e+00 - val_loss: 4.3226 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "4788/4788 [==============================] - 1s 226us/step - loss: 3.7325 - acc: 0.0000e+00 - val_loss: 4.3208 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "4788/4788 [==============================] - 1s 257us/step - loss: 3.7322 - acc: 0.0000e+00 - val_loss: 4.3225 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "4788/4788 [==============================] - 1s 225us/step - loss: 3.7323 - acc: 2.0886e-04 - val_loss: 4.3201 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "4788/4788 [==============================] - 1s 228us/step - loss: 3.7321 - acc: 0.0000e+00 - val_loss: 4.3223 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "4788/4788 [==============================] - 1s 232us/step - loss: 3.7322 - acc: 4.1771e-04 - val_loss: 4.3208 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 3.7319 - acc: 2.0886e-04 - val_loss: 4.3178 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "4788/4788 [==============================] - 1s 313us/step - loss: 3.7322 - acc: 2.0886e-04 - val_loss: 4.3210 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4788/4788 [==============================] - 1s 283us/step - loss: 3.7326 - acc: 2.0886e-04 - val_loss: 4.3190 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "4788/4788 [==============================] - 1s 267us/step - loss: 3.7322 - acc: 2.0886e-04 - val_loss: 4.3208 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "4788/4788 [==============================] - 1s 263us/step - loss: 3.7318 - acc: 2.0886e-04 - val_loss: 4.3229 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "4788/4788 [==============================] - 1s 240us/step - loss: 3.7317 - acc: 2.0886e-04 - val_loss: 4.3204 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "4788/4788 [==============================] - 1s 237us/step - loss: 3.7318 - acc: 2.0886e-04 - val_loss: 4.3200 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "4788/4788 [==============================] - 1s 243us/step - loss: 3.7316 - acc: 2.0886e-04 - val_loss: 4.3181 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "4788/4788 [==============================] - 1s 226us/step - loss: 3.7315 - acc: 2.0886e-04 - val_loss: 4.3210 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "4788/4788 [==============================] - 1s 254us/step - loss: 3.7320 - acc: 2.0886e-04 - val_loss: 4.3208 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "4788/4788 [==============================] - 1s 232us/step - loss: 3.7321 - acc: 2.0886e-04 - val_loss: 4.3224 - val_acc: 8.3472e-04\n",
      "Epoch 65/100\n",
      "4788/4788 [==============================] - 1s 256us/step - loss: 3.7315 - acc: 4.1771e-04 - val_loss: 4.3201 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "4788/4788 [==============================] - 1s 255us/step - loss: 3.7319 - acc: 4.1771e-04 - val_loss: 4.3194 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "4788/4788 [==============================] - 1s 233us/step - loss: 3.7322 - acc: 2.0886e-04 - val_loss: 4.3200 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 3.7311 - acc: 2.0886e-04 - val_loss: 4.3227 - val_acc: 8.3472e-04\n",
      "Epoch 69/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 3.7315 - acc: 2.0886e-04 - val_loss: 4.3183 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "4788/4788 [==============================] - 1s 242us/step - loss: 3.7312 - acc: 2.0886e-04 - val_loss: 4.3168 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "4788/4788 [==============================] - 1s 278us/step - loss: 3.7311 - acc: 4.1771e-04 - val_loss: 4.3174 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "4788/4788 [==============================] - 1s 234us/step - loss: 3.7305 - acc: 2.0886e-04 - val_loss: 4.3163 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "4788/4788 [==============================] - 2s 363us/step - loss: 3.7320 - acc: 2.0886e-04 - val_loss: 4.3149 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "4788/4788 [==============================] - 2s 379us/step - loss: 3.7316 - acc: 2.0886e-04 - val_loss: 4.3131 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "4788/4788 [==============================] - 1s 264us/step - loss: 3.7311 - acc: 2.0886e-04 - val_loss: 4.3210 - val_acc: 0.0017\n",
      "Epoch 76/100\n",
      "4788/4788 [==============================] - 1s 241us/step - loss: 3.7309 - acc: 2.0886e-04 - val_loss: 4.3184 - val_acc: 8.3472e-04\n",
      "Epoch 77/100\n",
      "4788/4788 [==============================] - 1s 219us/step - loss: 3.7304 - acc: 2.0886e-04 - val_loss: 4.3240 - val_acc: 8.3472e-04\n",
      "Epoch 78/100\n",
      "4788/4788 [==============================] - 1s 251us/step - loss: 3.7307 - acc: 2.0886e-04 - val_loss: 4.3111 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "4788/4788 [==============================] - 1s 226us/step - loss: 3.7309 - acc: 2.0886e-04 - val_loss: 4.3143 - val_acc: 8.3472e-04\n",
      "Epoch 80/100\n",
      "4788/4788 [==============================] - 1s 228us/step - loss: 3.7314 - acc: 2.0886e-04 - val_loss: 4.3173 - val_acc: 0.0017\n",
      "Epoch 81/100\n",
      "4788/4788 [==============================] - 1s 221us/step - loss: 3.7300 - acc: 2.0886e-04 - val_loss: 4.3224 - val_acc: 8.3472e-04\n",
      "Epoch 82/100\n",
      "4788/4788 [==============================] - 1s 224us/step - loss: 3.7302 - acc: 2.0886e-04 - val_loss: 4.3227 - val_acc: 8.3472e-04\n",
      "Epoch 83/100\n",
      "4788/4788 [==============================] - 1s 243us/step - loss: 3.7301 - acc: 0.0000e+00 - val_loss: 4.3097 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "4788/4788 [==============================] - 1s 250us/step - loss: 3.7313 - acc: 2.0886e-04 - val_loss: 4.3153 - val_acc: 8.3472e-04\n",
      "Epoch 85/100\n",
      "4788/4788 [==============================] - 1s 244us/step - loss: 3.7303 - acc: 2.0886e-04 - val_loss: 4.3168 - val_acc: 0.0017\n",
      "Epoch 86/100\n",
      "4788/4788 [==============================] - 1s 233us/step - loss: 3.7297 - acc: 6.2657e-04 - val_loss: 4.3154 - val_acc: 8.3472e-04\n",
      "Epoch 87/100\n",
      "4788/4788 [==============================] - 1s 265us/step - loss: 3.7295 - acc: 2.0886e-04 - val_loss: 4.3106 - val_acc: 8.3472e-04\n",
      "Epoch 88/100\n",
      "4788/4788 [==============================] - 1s 258us/step - loss: 3.7302 - acc: 2.0886e-04 - val_loss: 4.3122 - val_acc: 8.3472e-04\n",
      "Epoch 89/100\n",
      "4788/4788 [==============================] - 1s 264us/step - loss: 3.7296 - acc: 4.1771e-04 - val_loss: 4.3103 - val_acc: 8.3472e-04\n",
      "Epoch 90/100\n",
      "4788/4788 [==============================] - 1s 279us/step - loss: 3.7294 - acc: 2.0886e-04 - val_loss: 4.3174 - val_acc: 0.0017\n",
      "Epoch 91/100\n",
      "4788/4788 [==============================] - ETA: 0s - loss: 3.7574 - acc: 2.1259e-0 - 1s 267us/step - loss: 3.7290 - acc: 2.0886e-04 - val_loss: 4.3088 - val_acc: 8.3472e-04\n",
      "Epoch 92/100\n",
      "4788/4788 [==============================] - 1s 257us/step - loss: 3.7303 - acc: 2.0886e-04 - val_loss: 4.3104 - val_acc: 8.3472e-04\n",
      "Epoch 93/100\n",
      "4788/4788 [==============================] - 2s 334us/step - loss: 3.7304 - acc: 2.0886e-04 - val_loss: 4.3137 - val_acc: 8.3472e-04\n",
      "Epoch 94/100\n",
      "4788/4788 [==============================] - 1s 265us/step - loss: 3.7292 - acc: 2.0886e-04 - val_loss: 4.3150 - val_acc: 0.0017\n",
      "Epoch 95/100\n",
      "4788/4788 [==============================] - 1s 249us/step - loss: 3.7299 - acc: 2.0886e-04 - val_loss: 4.3162 - val_acc: 0.0017\n",
      "Epoch 96/100\n",
      "4788/4788 [==============================] - 1s 250us/step - loss: 3.7299 - acc: 2.0886e-04 - val_loss: 4.3162 - val_acc: 0.0017\n",
      "Epoch 97/100\n",
      "4788/4788 [==============================] - 1s 248us/step - loss: 3.7295 - acc: 4.1771e-04 - val_loss: 4.3195 - val_acc: 8.3472e-04\n",
      "Epoch 98/100\n",
      "4788/4788 [==============================] - 1s 253us/step - loss: 3.7307 - acc: 2.0886e-04 - val_loss: 4.3110 - val_acc: 8.3472e-04\n",
      "Epoch 99/100\n",
      "4788/4788 [==============================] - 1s 283us/step - loss: 3.7302 - acc: 4.1771e-04 - val_loss: 4.3139 - val_acc: 0.0017\n",
      "Epoch 100/100\n",
      "4788/4788 [==============================] - 1s 245us/step - loss: 3.7299 - acc: 2.0886e-04 - val_loss: 4.3091 - val_acc: 8.3472e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model = adam_model()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 10-fold Cross-Validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-324.66510259408585, total= 1.3min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=-308.01108700046956, total= 1.6min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=-2.470271030133077, total= 1.4min\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-296.28213679292963, total= 1.4min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-265.9296838317769, total= 1.4min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.132873106082413, total= 1.2min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-244.5500994015697, total= 1.3min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.927334952912601, total= 1.3min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.720715924648935, total= 1.4min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.654058054538076, total= 1.4min\n",
      "Results: Mean: -146.03 \n",
      " MSE: (143.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 13.8min finished\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold, verbose=3)\n",
    "print(\"Results: Mean: %.2f \\n MSE: (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-3.2657070942036497, total= 1.4min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=-2.8504705693765553, total= 1.8min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=-2.4385987406780005, total= 1.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-296.28213679292963, total= 1.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.622125271166705, total= 1.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.121874450244171, total= 1.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-5.249568546097414, total= 1.6min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.863695238744933, total= 2.0min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.761846803103801, total= 1.9min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-4.660782313267124, total= 1.7min\n",
      "Results: Mean: -33.31 \n",
      " MSE: (87.66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 16.7min finished\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasRegressor(build_fn=adam_model, epochs=100, batch_size=32, verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold, verbose=3)\n",
    "print(\"Results: Mean: %.2f \\n MSE: (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    # Compile model\n",
    "    opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-110249039.53255425, total= 1.6min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=-57872505.86310518, total= 1.7min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=-91790715.00500834, total= 1.9min\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-44561404.500834726, total= 1.8min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-38003319.19866444, total= 1.8min\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=-31767592.808013357, total= 1.8min\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=-38012490.68896321, total= 1.8min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-b1fe052ba8d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mabs_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: Mean: %.2f \\n MSE: (%.2f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_keras_initialized'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator = KerasRegressor(build_fn=abs_model, epochs=100, batch_size=32, verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold, verbose=3)\n",
    "print(\"Results: Mean: %.2f \\n MSE: (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
